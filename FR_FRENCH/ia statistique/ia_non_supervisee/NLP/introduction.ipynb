{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction au traitement du langage naturel (NLP) en utilisant des modèles non supervisée\n",
    "\n",
    "## Qu'est ce que NLP ?\n",
    "\n",
    "Nutural language processing ou traitement automatique du langage naturel est un domaine de l'intelligence artificielle qui se concentre sur l'interaction entre les ordinateurs et les humaisn en utilisant le langage naturel.\n",
    "\n",
    "Avant d'utiliser des modèles d'ia pour réaliser des projets, il est très important de choisir correctement ses données et de les prétraiter correctement.\n",
    "\n",
    "Il existe un processus pour pré-traiter ces données appelé nettoyage.\n",
    "\n",
    "## Prétraiter les données (Nettoyage)\n",
    "\n",
    "Il y a 6 étapes pour nettoyer les données :\n",
    "- La suppression des signes de ponctuation\n",
    "- La tokenization\n",
    "- La suppression des mots vides\n",
    "- Le stemming\n",
    "- La lemmatisation\n",
    "- Le Part Of Speech Tagging\n",
    "\n",
    "### La suppresion des signes de ponctuation\n",
    "\n",
    "On supprime les ponctuations des phrases, elles ne sont pas utiles car elles n'ajoutent pas de sens à la phrase, au langage.\n",
    "\n",
    "Exemple en code python :\n",
    "\n",
    "Dans un premier temps il faut charger notre jeu de donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UserName  ScreenName                      Location     TweetAt  \\\n",
      "0          3799       48751                        London  16-03-2020   \n",
      "1          3800       48752                            UK  16-03-2020   \n",
      "2          3801       48753                     Vagabonds  16-03-2020   \n",
      "3          3802       48754                           NaN  16-03-2020   \n",
      "4          3803       48755                           NaN  16-03-2020   \n",
      "...         ...         ...                           ...         ...   \n",
      "41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n",
      "41153     44952       89904                           NaN  14-04-2020   \n",
      "41154     44953       89905                           NaN  14-04-2020   \n",
      "41155     44954       89906                           NaN  14-04-2020   \n",
      "41156     44955       89907  i love you so much || he/him  14-04-2020   \n",
      "\n",
      "                                           OriginalTweet           Sentiment  \n",
      "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
      "1      advice Talk to your neighbours family to excha...            Positive  \n",
      "2      Coronavirus Australia: Woolworths to give elde...            Positive  \n",
      "3      My food stock is not the only one which is emp...            Positive  \n",
      "4      Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
      "...                                                  ...                 ...  \n",
      "41152  Airline pilots offering to stock supermarket s...             Neutral  \n",
      "41153  Response to complaint not provided citing COVI...  Extremely Negative  \n",
      "41154  You know itÂs getting tough when @KameronWild...            Positive  \n",
      "41155  Is it wrong that the smell of hand sanitizer i...             Neutral  \n",
      "41156  @TartiiCat Well new/used Rift S are going for ...            Negative  \n",
      "\n",
      "[41157 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"../../../../data/NLP/Corona_NLP_train.csv\", encoding='latin1')\n",
    "\n",
    "print(dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons retirer la ponctuation des tweets originals\n",
    "\n",
    "Pour se faire nous allons créer une fonction effacer_ponctuation qui va utiliser les regex pour effacer la ponctuation. On doit importer la librairie python pour gérer les regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effacer_ponctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite utiliser cette fonction pour les retirer les ponctuations de nos textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MeNyrbie Phil_Gahan Chrisitv httpstcoiFz9FAn2Pa and httpstcoxX6ghGFzCC and httpstcoI2NlzdxNo8', 'advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order', 'Coronavirus Australia Woolworths to give elderly disabled dedicated shopping hours amid COVID19 outbreak httpstcobInCA9Vp8P', 'My food stock is not the only one which is empty\\r\\r\\n\\r\\r\\nPLEASE dont panic THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need \\r\\r\\nStay calm stay safe\\r\\r\\n\\r\\r\\nCOVID19france COVID_19 COVID19 coronavirus confinement Confinementotal ConfinementGeneral httpstcozrlG0Z520j', 'Me ready to go at supermarket during the COVID19 outbreak\\r\\r\\n\\r\\r\\nNot because Im paranoid but because my food stock is litteraly empty The coronavirus is a serious thing but please dont panic It causes shortage\\r\\r\\n\\r\\r\\nCoronavirusFrance restezchezvous StayAtHome confinement httpstcousmuaLq72n']\n"
     ]
    }
   ],
   "source": [
    "text_sans_ponctuation = [effacer_ponctuation(text) for text in dataframe['OriginalTweet']]\n",
    "\n",
    "print(text_sans_ponctuation[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après la suppression des ponctuations nous pouvons passer à la tokenization\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "La tokenization permet de séparer les mots de la chaîne de caractère en liste de mot séparés. Cela sera très important pour le traitement de nos données dans les différents modèles.\n",
    "\n",
    "Pour réaliser la tokenization on peut utiliser diverse librarie comme NLTK spécialisé dans le traitement du langage naturel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/huynh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# données nécessaire pour réaliser la tokenization avec la lib NLTK\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ici écrire la fonction tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_sans_ponctuation):\n",
    "    texte_tokenize = []\n",
    "    for text in text_sans_ponctuation:\n",
    "        texte_tokenize.append(word_tokenize(text))\n",
    "    return texte_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advice', 'Talk', 'to', 'your', 'neighbours', 'family', 'to', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'with', 'phone', 'numbers', 'of', 'neighbours', 'schools', 'employer', 'chemist', 'GP', 'set', 'up', 'online', 'shopping', 'accounts', 'if', 'poss', 'adequate', 'supplies', 'of', 'regular', 'meds', 'but', 'not', 'over', 'order']\n"
     ]
    }
   ],
   "source": [
    "texte_tokenize = tokenize(text_sans_ponctuation)\n",
    "\n",
    "print(texte_tokenize[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après la tokenization on va réaliser la suppression des mots vides\n",
    "\n",
    "### Suppresion des mots vides\n",
    "\n",
    "La suppression des mots vides permet de retirer des mots qui sont utilisés pour la structure du langage (dit langage structurant) mais qui ne contribuent pas en aucune manière à son contenu ou sa compréhension.\n",
    "\n",
    "Par exemple le, la, un, une\n",
    "\n",
    "Pour réaliser ça on peut utiliser utiliser la liste des stop_words de la librairie nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/huynh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# données pour avoir une liste de stop words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite nous allons définir notre fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppression_mots_vides(texte_tokenize):\n",
    "    texte_sans_mots_vides= []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for texte in texte_tokenize:\n",
    "        texte_sans_mots_vides.append([mot for mot in texte if mot not in stop_words])\n",
    "    return texte_sans_mots_vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advice', 'Talk', 'neighbours', 'family', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'phone', 'numbers', 'neighbours', 'schools', 'employer', 'chemist', 'GP', 'set', 'online', 'shopping', 'accounts', 'poss', 'adequate', 'supplies', 'regular', 'meds', 'order']\n"
     ]
    }
   ],
   "source": [
    "text_sans_mots_vides = suppression_mots_vides(texte_tokenize)\n",
    "\n",
    "print(text_sans_mots_vides[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après la suppression des mots vides on peut réaliser un prétraitement permettant de mettre les mots en minuscules\n",
    "\n",
    "Voici une fonction pour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mettre_en_minuscule(text_sans_mots_vides):\n",
    "    text_minuscule = []\n",
    "    for text in text_sans_mots_vides:\n",
    "        text_minuscule.append([mot.lower() for mot in text])\n",
    "    return text_minuscule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advice', 'talk', 'neighbours', 'family', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'phone', 'numbers', 'neighbours', 'schools', 'employer', 'chemist', 'gp', 'set', 'online', 'shopping', 'accounts', 'poss', 'adequate', 'supplies', 'regular', 'meds', 'order']\n"
     ]
    }
   ],
   "source": [
    "text_minuscule = mettre_en_minuscule(text_sans_mots_vides)\n",
    "\n",
    "print(text_minuscule[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le stemming\n",
    "\n",
    "Le stemming est un processus pour supprimer les affixes de mots et les laisser sous une forme canonique invariante on peut aussi dire qu'il met les mots sous forme de \"racine\". Les mots n'auront pas forcement de sens propre dans notre langage.\n",
    "\n",
    "Voici la fonction pour réaliser cela. Nous continuerons à utiliser la librairie nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text_minuscule):\n",
    "    text_stemming = []\n",
    "    for text in text_minuscule:\n",
    "        text_stemming.append([PorterStemmer().stem(mot) for mot in text])\n",
    "    return text_stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advic', 'talk', 'neighbour', 'famili', 'exchang', 'phone', 'number', 'creat', 'contact', 'list', 'phone', 'number', 'neighbour', 'school', 'employ', 'chemist', 'gp', 'set', 'onlin', 'shop', 'account', 'poss', 'adequ', 'suppli', 'regular', 'med', 'order']\n"
     ]
    }
   ],
   "source": [
    "text_stemming = stemming(text_minuscule)\n",
    "\n",
    "print(text_stemming[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La lemmatisation\n",
    "\n",
    "La lemmatisation c'est un processus algorithmique pour amener un mot dans sa forme de dictionnaire dit \"Lemma\".\n",
    "\n",
    "_Il est préférable de réaliser la lemmatisation à la place du stemming si le texte (les données) fournis sont propres avec peu d'erreur. En effet par exemple le stemming ne fera pas la différence entre le mot est (être) et le mot est comme Grand-Est._\n",
    "\n",
    "_Il faut que le texte soit propre car sinon le sens du mot par exemple s'il y a des fautes d'orthographe peut être mal interprété et donc changer le sens de celui ci après lemmatisation. (ça peut être le cas aussi avec des problèmes de grammaire et de syntaxes ou autres ...)_\n",
    "\n",
    "⚠️ La lemmatisation doit se faire avant de retirer les stop words ! Il faut faire le POS tagging pour réaliser la lemmatisation si on veut que la lemmetisation soit meilleur que le stemming.\n",
    "\n",
    "_Ici on illustre seulement comment réaliser les techniques dont on aura besoin dans le futur pour prétraiter les données._\n",
    "\n",
    "Voici une fonction pour réaliser la lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/huynh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatisation(text_minuscule):\n",
    "    text_lemma = []\n",
    "    for text in text_minuscule:\n",
    "        text_lemma.append([WordNetLemmatizer().lemmatize(mot) for mot in text])\n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advice', 'talk', 'neighbour', 'family', 'exchange', 'phone', 'number', 'create', 'contact', 'list', 'phone', 'number', 'neighbour', 'school', 'employer', 'chemist', 'gp', 'set', 'online', 'shopping', 'account', 'po', 'adequate', 'supply', 'regular', 'med', 'order']\n"
     ]
    }
   ],
   "source": [
    "text_lemma = lemmatisation(text_minuscule)\n",
    "\n",
    "print(text_lemma[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Of Speech Tagging (POS tagging)\n",
    "\n",
    "Le Part Of Speech Tagging est l'étape qui permet de mettre une étique à chaque mot d'une phrase avec sa classe grammaticale, comme un adjectif, un nom, un verbe, adverbe et présition etc ...\n",
    "\n",
    "Dans la plus part des fonctions qui permmettent la lemmatisation le POS Tagging est fait dans la foulé. En revanche sur la librairie nltk avec la fonction qu'on a utilisé le POS Tagging n'est pas réalisé. Je vous invite donc à lire les documentations pour les librairies que vous voulez utiliser pour le pré-traitement.\n",
    "\n",
    "Le POS se fait juste après l'étape de la tokenization donc nous allons reprendre à cette étape.\n",
    "\n",
    "Nous allons donc réaliser cette étape :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/huynh/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    texte_tag = []\n",
    "    for texte in text:\n",
    "        texte_tag.append(nltk.pos_tag(texte))\n",
    "    return texte_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('advice', 'NN'), ('Talk', 'NN'), ('to', 'TO'), ('your', 'PRP$'), ('neighbours', 'NNS'), ('family', 'NN'), ('to', 'TO'), ('exchange', 'VB'), ('phone', 'NN'), ('numbers', 'NNS'), ('create', 'VBP'), ('contact', 'JJ'), ('list', 'NN'), ('with', 'IN'), ('phone', 'NN'), ('numbers', 'NNS'), ('of', 'IN'), ('neighbours', 'NNS'), ('schools', 'NNS'), ('employer', 'VBP'), ('chemist', 'JJ'), ('GP', 'NNP'), ('set', 'VBD'), ('up', 'RP'), ('online', 'JJ'), ('shopping', 'NN'), ('accounts', 'NNS'), ('if', 'IN'), ('poss', 'JJ'), ('adequate', 'JJ'), ('supplies', 'NNS'), ('of', 'IN'), ('regular', 'JJ'), ('meds', 'NNS'), ('but', 'CC'), ('not', 'RB'), ('over', 'IN'), ('order', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "texte_tag = pos_tagging(texte_tokenize)\n",
    "\n",
    "print(texte_tag[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalement c'est après cette étape qu'on réalise la lemmatisation. Dans ce cas il faut réaliser une nouvelle fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging_lemmatization(text):\n",
    "    text_lemma = []\n",
    "    for texte in text:\n",
    "        lemma_tags = []\n",
    "        for token, tag in texte:\n",
    "            if tag.startswith('N'):\n",
    "                lemma = WordNetLemmatizer().lemmatize(token, pos='n')\n",
    "            elif tag.startswith('V'):\n",
    "                lemma = WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "            elif tag.startswith('J'):\n",
    "                lemma = WordNetLemmatizer().lemmatize(token, pos='a')\n",
    "            elif tag.startswith('R'):\n",
    "                lemma = WordNetLemmatizer().lemmatize(token, pos='r')\n",
    "            else:\n",
    "                lemma = WordNetLemmatizer().lemmatize(token)\n",
    "            lemma_tags.append(lemma)\n",
    "        text_lemma.append(lemma_tags)\n",
    "    return text_lemma\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advice', 'Talk', 'to', 'your', 'neighbour', 'family', 'to', 'exchange', 'phone', 'number', 'create', 'contact', 'list', 'with', 'phone', 'number', 'of', 'neighbour', 'school', 'employer', 'chemist', 'GP', 'set', 'up', 'online', 'shopping', 'account', 'if', 'poss', 'adequate', 'supply', 'of', 'regular', 'med', 'but', 'not', 'over', 'order']\n"
     ]
    }
   ],
   "source": [
    "text_lemma = pos_tagging_lemmatization(texte_tag)\n",
    "\n",
    "print(text_lemma[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En résumé\n",
    "\n",
    "Il faut réaliser les étapes si on veut faire de la lemmatisation dans cette ordre :\n",
    "\n",
    "Tokenisation > POS Tagging (si non compris dans la lemmatisation) > Lemmatisation > Suppression ponctuation > Mettre dans le meme caste (minuscule ou majuscule) > Suppression des stops words \n",
    "\n",
    "Sinon faire :\n",
    "\n",
    "Suppression ponctuation > Tokenisation > Mettre dans le meme caste > Suppression des stops words > Stemming\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Ce type de prétraitement de donner concerne le text-mining voir le cours sur text-mining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
